{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding, AdamW, get_scheduler\n",
    "from vncorenlp import VnCoreNLP\n",
    "\n",
    "import loss\n",
    "from CustomSoftmaxModel import CustomModelSoftmax\n",
    "from metrics import metric\n",
    "from preprocessing.NewsPreprocessing import Preprocess\n",
    "from utils import pred_to_label, update_model\n",
    "from visualization import Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "seed = 19133022\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "589d7891015b4c81"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "rdrsegmenter = VnCoreNLP(\"preprocessing/vncorenlp/VnCoreNLP-1.1.1.jar\",\n",
    "                         annotators=\"wseg\", max_heap_size='-Xmx500m')\n",
    "# Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\")\n",
    "# Load datasets & Preprocess\n",
    "preprocess = Preprocess(tokenizer, rdrsegmenter)\n",
    "inputs = {'train': r\"./data/training_data/train_datasets.csv\",\n",
    "          'test': r\"./data/training_data/test_datasets.csv\"}\n",
    "tokenized_datasets = preprocess.run(load_dataset('csv', data_files=inputs))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c5808af5c17d221f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "learning_rate = 5e-5\n",
    "batch_size = 32"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8ec8162ed394847f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Data loader\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "train_dataloader = DataLoader(tokenized_datasets[\"train\"], batch_size=batch_size, collate_fn=data_collator, shuffle=True)\n",
    "test_dataloader = DataLoader(tokenized_datasets[\"test\"], batch_size=batch_size, collate_fn=data_collator)\n",
    "# Model\n",
    "phobert = CustomModelSoftmax(\"vinai/phobert-base\")\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "phobert.to(device)\n",
    "# Optimizer\n",
    "optimizer = AdamW(phobert.parameters(), lr=learning_rate)\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "lr_scheduler = get_scheduler('linear', optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "16185884479c0b0e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def evaluation(model, val_dataloader):\n",
    "    valid = metric()\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in val_dataloader:\n",
    "            inputs = {'input_ids': batch['input_ids'].to(device),\n",
    "                      'attention_mask': batch['attention_mask'].to(device)}\n",
    "            outputs_classifier, outputs_regressor = model(**inputs)\n",
    "            # loss\n",
    "            classifier_loss = loss.classifier(outputs_classifier, batch['labels_classifier'].to(device).float())\n",
    "            softmax_loss = loss.softmax(outputs_regressor, batch['labels_regressor'].to(device).float(), device)\n",
    "            mix_loss = classifier_loss + softmax_loss\n",
    "            outputs_classifier = outputs_classifier.cpu().numpy()\n",
    "            outputs_regressor = outputs_regressor.cpu().numpy()\n",
    "            outputs_regressor = outputs_regressor.argmax(axis=-1) + 1\n",
    "            outputs = pred_to_label(outputs_classifier, outputs_regressor)\n",
    "            # update loss\n",
    "            y_true = batch['labels_regressor'].numpy()\n",
    "            valid.classifier_loss.update(classifier_loss.item())\n",
    "            valid.regressor_loss.update(softmax_loss.item())\n",
    "            valid.loss.update(mix_loss.item())\n",
    "            valid.acc.update(np.round(outputs), y_true)\n",
    "            valid.f1_score.update(np.round(outputs), y_true)\n",
    "            valid.r2_score.update(np.round(outputs), y_true)\n",
    "    return valid"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e3b93c3f387577bb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_log = Visualization()\n",
    "val_log = Visualization()\n",
    "best_score = -1\n",
    "for epoch in range(num_epochs):\n",
    "    train_metrics = metric()\n",
    "    phobert.train()\n",
    "    for batch in train_dataloader:\n",
    "        inputs = {'input_ids': batch['input_ids'].to(device),\n",
    "                  'attention_mask': batch['attention_mask'].to(device)}\n",
    "        outputs_classifier, outputs_regressor = phobert(**inputs)\n",
    "        sigmoid_focal_loss = loss.sigmoid_focal(outputs_classifier, \n",
    "                                                batch['labels_classifier'].to(device).float(),\n",
    "                                                alpha=-1, gamma=1, reduction='mean')\n",
    "        softmax_loss = loss.softmax(outputs_regressor, \n",
    "                                    batch['labels_regressor'].to(device).float(), device)\n",
    "        mix_loss = 10 * sigmoid_focal_loss + softmax_loss\n",
    "        optimizer.zero_grad()\n",
    "        mix_loss.backward()\n",
    "        optimizer.step()\n",
    "        with torch.no_grad():\n",
    "            outputs_classifier = outputs_classifier.cpu().numpy()\n",
    "            outputs_regressor = outputs_regressor.cpu().numpy()\n",
    "            outputs_regressor = outputs_regressor.argmax(axis=-1) + 1\n",
    "            outputs = pred_to_label(outputs_classifier, outputs_regressor)\n",
    "            y_true = batch['labels_regressor'].numpy()\n",
    "            train_metrics.sigmoid_focal_loss.update(sigmoid_focal_loss.item())\n",
    "            train_metrics.regressor_loss.update(softmax_loss.item())\n",
    "            train_metrics.loss.update(mix_loss.item())\n",
    "            train_metrics.acc.update(np.round(outputs), y_true)\n",
    "            train_metrics.f1_score.update(np.round(outputs), y_true)\n",
    "            train_metrics.r2_score.update(np.round(outputs), y_true)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "407a49ebbb8f6254"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'category'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[17], line 23\u001B[0m\n\u001B[0;32m     21\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i, r \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(factors):\n\u001B[0;32m     22\u001B[0m     output[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mresults\u001B[39m\u001B[38;5;124m\"\u001B[39m][r] \u001B[38;5;241m=\u001B[39m factors[r][\u001B[38;5;28mint\u001B[39m(predict_results[i])]\n\u001B[1;32m---> 23\u001B[0m \u001B[43moutput\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mcategory\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\n",
      "\u001B[1;31mKeyError\u001B[0m: 'category'"
     ]
    }
   ],
   "source": [
    "output = {\n",
    "        \"review\": \"sentence\",\n",
    "        \"results\": {}\n",
    "    }\n",
    "\n",
    "factors = {\"category\": [\"Không xác định\",\n",
    "                        \"Nông sản lúa, gạo, nếp... hoặc từ sản phẩm lúa\", \n",
    "                        \"Nông sản cà phê\", \"Nông sản cao su\"],\n",
    "           \"price\": [\"Không xác định\", \"Giảm\", \"Ổn định\", \"Tăng\"], \n",
    "           \"market\":[\"Không xác định\", \n",
    "                     \"Nguồn cung lớn hơn nhu cầu\", \n",
    "                     \"Nguồn cung và cầu ổn định\", \n",
    "                     \"Nhu cầu lớn hơn Nguồn cung\"], \n",
    "           \"polices\":[\"Không xác định\", \n",
    "                      \"Chính sách\", \"Hiệp định\", \"Khác\"], \n",
    "           \"internal\":[\"Không xác định\", \"Liên quan đến sản lượng nông sản\", \n",
    "                       \"Liên quan đến chất lượng nông sản.\", \"Chi phí sản suất liên quan\"], \n",
    "           \"external\":[\"Không xác định\", \"Dịch bệnh\", \"Thiên tai\", \"Khủng hoảng\"]\n",
    "           }\n",
    "predict_results = [0,1,2,3,0,1]\n",
    "for i, r in enumerate(factors):\n",
    "    output[\"results\"][r] = factors[r][int(predict_results[i])]\n",
    "output['category']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-03T09:53:59.338532400Z",
     "start_time": "2023-12-03T09:53:59.294462200Z"
    }
   },
   "id": "ffe75debe8c575e4"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
